{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1524f72",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "### Idea: Try to average all the probabilites of the heads and take the top 5 heads "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "64c5b4b9-4d92-43e0-8eb8-56d390e4b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import seaborn as sns\n",
    "from circuitsvis.attention import attention_heads, attention_patterns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import json \n",
    "from collections import defaultdict\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e6cfde61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "torch.set_grad_enabled(False) #to disable gradients -> faster computiations\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# Ensure GPU acceleration is enabled on Mac\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "mod = None\n",
    "tokenizer = None\n",
    "META_LLAMA_3_2_3B = \"meta-llama/Llama-3.2-3B\"\n",
    "GOOGLE_GEMMA_2_2B = \"google/gemma-2-2b\"\n",
    "dataset = {}\n",
    "CSV_PATH_DATASET = \"dataset/examples.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ce387c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [META_LLAMA_3_2_3B] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202a4862",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5f81b639",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_name: str, tokenizer_name: str = None):\n",
    "    if not tokenizer_name:\n",
    "        tokenizer_name = model_name\n",
    "    # Initialize model and tokenizer\n",
    "    global model\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "    if not tokenizer_name:\n",
    "        tokenizer_name = model_name\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "3250539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_to_csv: str):\n",
    "    # Check if the file at the given path exists\n",
    "    if os.path.exists(path_to_csv):\n",
    "        df = pd.read_csv(path_to_csv)\n",
    "    else:\n",
    "        print(\"File does not exist.\")\n",
    "        exit(1)\n",
    "\n",
    "    global dataset \n",
    "    dataset = df\n",
    "    \n",
    "    # Create a new column \"token_probability\" for saving up the probabilites of the studied token for all prompts. Initially, 0.\n",
    "    dataset[\"token_probability_true_sentence\"] = 0\n",
    "    dataset[\"token_probability_false_sentence\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ddc14479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_colored_separator(color=\"\\033[94m\", char=\"=\", length=150, prints_enabled: bool = False):\n",
    "    if prints_enabled:\n",
    "        reset = \"\\033[0m\"  # Reset color\n",
    "        print(f\"{color}{char * length}{reset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b66827c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the model's output after feeding it with a prompt concatenated prompt_repetitions times and the concatenated prompt tensor\n",
    "def feed_forward(true_sentence: str, false_sentence: str, prompt_repetitions: int = 1, prints_enabled: bool = False):\n",
    "    print_colored_separator(prints_enabled)\n",
    "    # Before proceeding, check that the true_sentence and false_sentence contain the same amount of tokens after tokenizing them. \n",
    "    # Important!: BOS token is usually not included for counting the tokens of a sentence, when indexing .shape[...]\n",
    "    true_sentence_token_n = tokenizer(true_sentence, return_tensors=\"pt\")[\"input_ids\"][0].shape[0]\n",
    "    false_sentence_token_n = tokenizer(false_sentence, return_tensors=\"pt\")[\"input_ids\"][0].shape[0]\n",
    "    if true_sentence_token_n != false_sentence_token_n:\n",
    "        return None, None, None\n",
    "    \n",
    "    # Extract all the words except the last one, split by space.  \n",
    "    sentence_without_last_token = \"\".join(true_sentence.rsplit(\" \", 1)[:-1])\n",
    "    # Append the sentence without the last token to the prompt, starting with the true_sentence. This is one-shot learning.\n",
    "    # Add space token to avoid that the point token \".\" gets tokenized together with the beginning of the next sentence.\n",
    "    prompt = true_sentence + \"\\n\" + false_sentence + \"\\n\" + sentence_without_last_token\n",
    "    token_sequence = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # print(f\"prompt: {prompt}\\ntoken_sequence: {token_sequence}\\nNumber of tokens: {len(token_sequence['input_ids'][0])}\")\n",
    "    tokens = token_sequence[\"input_ids\"][0]\n",
    "    \n",
    "    # Feed forward to the model\n",
    "    global model\n",
    "    out = model(tokens.unsqueeze(0).to(model.device), return_dict=True, output_attentions=True)\n",
    "    # Return the output of the model, the tokenized prompt, number of tokens from the sentences (both sentences should have the same amount of tokens at this point)\n",
    "    return out, tokens, true_sentence_token_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8df7ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_induction_mask_with_plotly(induction_mask, induction_mask_text, prompt):\n",
    "    # Create a Heatmap with the numeric mask (z) and attach the text\n",
    "    heatmap = go.Heatmap(\n",
    "        z=induction_mask,\n",
    "        text=induction_mask_text, \n",
    "        hoverinfo='text',  # Only show the text on hover\n",
    "        colorscale='Blues',  # Choose any Plotly colorscale you like\n",
    "        showscale=True\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[heatmap])\n",
    "\n",
    "    # Make the squares actually square by linking x/y scales\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(scaleanchor=\"y\", scaleratio=1),\n",
    "        yaxis=dict(autorange=\"reversed\"),  # Reverse y-axis so row 0 is at top\n",
    "        title=f\"Induction Mask for prompt: {prompt}\\n\"\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "645dcb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attention_mask(token_sequence: torch.Tensor, token_number_sentence: int, show_induction_mask: bool = False, prints_enabled: bool = False):\n",
    "    print_colored_separator(prints_enabled)\n",
    "    sequence_length = token_sequence.shape[0]\n",
    "    induction_mask = torch.zeros(sequence_length, sequence_length).to(float)\n",
    "    induction_mask_text = np.full((sequence_length, sequence_length), \"\", dtype=object)\n",
    "\n",
    "    # Start at the beginning of the second sentence (+1 since BOS token was not counted). \n",
    "    for i in range(token_number_sentence + 1, sequence_length):\n",
    "        if token_sequence[i] not in token_sequence[:i]:\n",
    "            continue\n",
    "        for j in range(i):\n",
    "            if token_sequence[i] == token_sequence[j]:\n",
    "                induction_mask[i, j + 1] = 1 \n",
    "                # Encode to show raw strings (show e.g. new lines tokens)\n",
    "                induction_mask_text[i, j + 1] = tokenizer.decode(token_sequence[i]).encode('unicode_escape').decode('utf-8') + \"/\" + tokenizer.decode(token_sequence[j + 1]).encode('unicode_escape').decode('utf-8')\n",
    "    \n",
    "    if show_induction_mask:\n",
    "        # print(\"Induction Mask:\\n\")\n",
    "        # print(induction_mask)\n",
    "        # print()\n",
    "        # print(\"Induction Mask plot:\\n\")\n",
    "        # plt.imshow(induction_mask)\n",
    "        # plt.show()\n",
    "        plot_induction_mask_with_plotly(induction_mask, induction_mask_text, prompt=tokenizer.decode(token_sequence))\n",
    "    return induction_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbdedb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_induction_head_scores(token_sequence: torch.Tensor, induction_mask: torch.Tensor, model_output):\n",
    "    num_heads = model.config.num_attention_heads\n",
    "    num_layers = model.config.num_hidden_layers\n",
    "    sequence_length = token_sequence.shape[0]\n",
    "\n",
    "    induction_scores = torch.zeros(num_layers, num_heads)\n",
    "    tril = torch.tril_indices(sequence_length, sequence_length) # gets the indices of elements on and below the diagonal\n",
    "    induction_flat = induction_mask[tril[0], tril[1]].flatten()\n",
    "    \n",
    "    for layer in range(num_layers):\n",
    "        for head in range(num_heads):\n",
    "            pattern = model_output[\"attentions\"][layer][0][head].to(float)\n",
    "            pattern_flat = pattern[tril[0], tril[1]].flatten()\n",
    "            score = (induction_flat @ pattern_flat) / pattern_flat.sum()\n",
    "            induction_scores[layer, head] = score\n",
    "    return induction_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f76d6c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap(induction_scores: torch.Tensor):\n",
    "    print_colored_separator()\n",
    "    _, ax = plt.subplots()\n",
    "    print(\"Heatmap of induction scores across heads and layers: \\n\")\n",
    "    sns.heatmap(induction_scores, cbar_kws={\"label\": \"Induction Head Score\"}, ax=ax)\n",
    "    ax.set_ylabel(\"Layer #\")\n",
    "    ax.set_xlabel(\"Head #\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "06e9893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_filter_high_scoring_induction_heads(induction_scores: torch.Tensor, model_output: any, show_induction_heads: bool = False, prints_enabled: bool = False): \n",
    "    print_colored_separator(prints_enabled)\n",
    "    \n",
    "    # Get flattened indices sorted by scores in descending order\n",
    "    sorted_flat_indices = torch.argsort(induction_scores.flatten(), descending=True)\n",
    "\n",
    "    # Convert flattened indices to 2D indices\n",
    "    sorted_indices = torch.unravel_index(sorted_flat_indices, induction_scores.shape)\n",
    "\n",
    "    # Stack the row and column indices for final output\n",
    "    sorted_indices = torch.stack(sorted_indices, dim=1)\n",
    "\n",
    "\n",
    "    if show_induction_heads:\n",
    "        print(\"Top 5 Induction Heads with the highest induction score - Descending order\\n\")\n",
    "        for layer, head in sorted_indices[:5]:\n",
    "            induction_score = induction_scores[layer][head]\n",
    "            print(f\"Layer: {layer}\\nHead: {head}\\nInduction Score: {induction_score}\")\n",
    "            plt.imshow(model_output[\"attentions\"][layer][0][head].cpu().float())\n",
    "            plt.show()\n",
    "            print()\n",
    "    return sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "f383db80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_probability_extraction(head_indices: torch.Tensor, models_output: any, token_number_sentence: int, prints_enabled: bool = False):\n",
    "    result_true_sentence = {}\n",
    "    result_false_sentence = {}\n",
    "    for idx in head_indices:\n",
    "        print_colored_separator(prints_enabled)\n",
    "        layer, head = idx\n",
    "        probs = models_output[\"attentions\"][layer][0][head]\n",
    "\n",
    "        # Extract probability of the specified token\n",
    "        sequence_length = probs.shape[0]\n",
    "        # First index is y-axis, second is x-axis from the source destination diagram.\n",
    "        # sequence_length - 1 because we want to index the last token of a sequence. \n",
    "        # token_number_sentence - 1 because we skip the newline at the end of each sentence. \n",
    "        probability_token_true_sentence = probs[sequence_length - 1, token_number_sentence - 1].item() \n",
    "        probability_token_false_sentence = probs[sequence_length - 1, 2 * token_number_sentence - 1].item() \n",
    "\n",
    "        # Results for token from true_sentence and false_sentence at current layer and head\n",
    "        result_true_sentence[f\"L{layer}_H{head}\"] = probability_token_true_sentence \n",
    "        result_false_sentence[f\"L{layer}_H{head}\"] = probability_token_false_sentence\n",
    "    return json.dumps(result_true_sentence), json.dumps(result_false_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "61f17ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_probability(token_probability: int, example_id: int, column_name_probability: str, prints_enabled: bool = False):\n",
    "    if dataset.empty:\n",
    "        raise Exception(\"Dataset is empty\")\n",
    "    \n",
    "    print_colored_separator(prints_enabled)\n",
    "    # Log the probability of the token into its corresponding row and column in the dataset.\n",
    "    dataset.loc[dataset[\"example_id\"] == example_id, f\"{column_name_probability}\"] = token_probability \n",
    "    if prints_enabled:\n",
    "        print(f\"Saved probability for token from example_id: {example_id}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "acd1ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention_visualizations(head_indices: torch.Tensor, token_sequence: torch.Tensor, models_output):\n",
    "    # Display attention diagrams\n",
    "    tokens_vis = tokenizer.tokenize(tokenizer.decode(token_sequence.squeeze()))\n",
    "    layer, head = head_indices[0]\n",
    "    return attention_patterns(tokens_vis, models_output[\"attentions\"][layer][0]), attention_heads(models_output[\"attentions\"][layer][0], tokens_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c7e6801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot_results(save_path: str, data):\n",
    "\n",
    "    plot_df = pd.DataFrame(data=h_average)\n",
    "    # Ensure \"Type\" column is formatted correctly to avoid duplicate legend entries\n",
    "    plot_df[\"Type\"] = plot_df[\"Type\"].astype(str).str.strip()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    ax = sns.barplot(data=plot_df, x=\"Head\", y=\"Probability\", hue=\"Type\")\n",
    "    ax.get_figure().savefig(f\"{save_path}-results-plot.png\", dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "efeb312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_across_heads(head_set, top_k_heads: int):\n",
    "    h_sum = defaultdict(float)\n",
    "    h_count = defaultdict(int)\n",
    "    for row in head_set:\n",
    "        for key, value in row.items():\n",
    "            h_sum[key] = h_sum[key] + value\n",
    "            h_count[key] = h_count[key] + 1\n",
    "\n",
    "    h_average = {h: h_sum[h] / h_count[h] for h in h_sum}\n",
    "    # Sort the values of the heads in descending order. Slice the top k heads.\n",
    "    sorted_heads = dict(sorted(h_average.items(), key=itemgetter(1), reverse=True)[:top_k_heads])\n",
    "    return sorted_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0fe104aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_average_statistic(slice_top_k_heads: int):\n",
    "    # Get the list of heads\n",
    "    avg_head_true_tokens = get_average_across_heads(dataset[\"token_probability_true_sentence\"].apply(json.loads), top_k_heads=slice_top_k_heads)\n",
    "    avg_head_false_tokens = get_average_across_heads(dataset[\"token_probability_false_sentence\"].apply(json.loads), top_k_heads=slice_top_k_heads)\n",
    "\n",
    "    print(\"this is the avg_true\", avg_head_true_tokens)\n",
    "    print(\"this is the avg_false\", avg_head_false_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4045f3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_result_csv(model_name: str, dataset_csv_file_path: str):\n",
    "    global dataset\n",
    "    model_name_folder = model_name.split(\"/\")\n",
    "    folder_path = os.path.dirname(dataset_csv_file_path) + \"/\" + model_name_folder[0] \n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    model_and_path = f\"{model_name_folder[-1]}-results.csv\"\n",
    "    new_file_path = os.path.join(folder_path, model_and_path)\n",
    "    dataset.to_csv(new_file_path, index=False)\n",
    "\n",
    "    # Save plot for the current dataset \n",
    "    model_and_path_image = os.path.join(folder_path, model_name_folder[-1])\n",
    "    return model_and_path_image\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "054eec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model():\n",
    "    global model\n",
    "    del model\n",
    "    gc.collect()\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.empty_cache()  # Clear MPS GPU memory\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0d07b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(true_sentence: str, false_sentence: str):\n",
    "    models_output, token_sequence, token_number_sentence = feed_forward(true_sentence=true_sentence, false_sentence=false_sentence)\n",
    "    if token_sequence is None: \n",
    "        # true_sentence and false_sentence have different number of tokens after tokenizing them. \n",
    "        return None, None\n",
    "    # print(\"Token Sequence: \", token_sequence)\n",
    "    # print(\"Token Sequence number: \", token_sequence.shape[0])\n",
    "    # print(\"Token number sentence: \", token_number_sentence)\n",
    "    induction_mask = create_attention_mask(token_sequence=token_sequence, token_number_sentence=token_number_sentence)\n",
    "    induction_scores = compute_induction_head_scores(token_sequence=token_sequence, induction_mask=induction_mask, model_output=models_output)\n",
    "    # create_heatmap(induction_scores=induction_scores)\n",
    "\n",
    "    return token_probability_true_sentence, token_probability_false_sentence \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f39ac44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply processing and store results\n",
    "def process_row(row):\n",
    "    result_true, result_false = run_experiment(true_sentence=row[\"true_sentence\"], \n",
    "                            false_sentence=row[\"false_sentence\"])\n",
    "    # Check if result is None or invalid\n",
    "    if result_true is None:\n",
    "        return pd.Series([pd.NA, pd.NA])  # Mark for removal\n",
    "    return pd.Series([result_true, result_false]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "95b22b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_suite(dataset_csv_file_path: str, llm_models: list, prompt_repetitions: int = 1):\n",
    "    for mod in llm_models: \n",
    "        print(f\"Using device: {torch.device('mps') if torch.backends.mps.is_available() else 'cpu'}\")\n",
    "        initialize_model(model_name=mod, tokenizer_name=mod)\n",
    "        load_dataset(path_to_csv=dataset_csv_file_path)\n",
    "        \n",
    "        # Use apply() in a vectorized manner to pass both columns\n",
    "        global dataset\n",
    "        dataset[[\"token_probability_true_sentence\", \"token_probability_false_sentence\"]] = dataset.apply(\n",
    "            lambda row: process_row(row),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "        # Drop rows where `process_row()` returned NaN\n",
    "        dataset.dropna(subset=[\"token_probability_true_sentence\", \"token_probability_false_sentence\"], inplace=True)\n",
    "\n",
    "        # Create CSV result files saved in folders respective to the used LLM.\n",
    "        model_image_path = save_result_csv(model_name=mod, dataset_csv_file_path=dataset_csv_file_path) \n",
    "\n",
    "        # Process the average probabilities across heads\n",
    "        # create_average_statistic(slice_top_k_heads=5)\n",
    "\n",
    "        # Plot the results\n",
    "        # save_plot_results(save_path=model_image_path)\n",
    "\n",
    "        # Delete the model loaded in memory\n",
    "        delete_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ef3f7f",
   "metadata": {},
   "source": [
    "### Experiment Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "23719aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your current working directory: /Users/ivannaranjo/Documents/Helmholtz/experiments/hlmz-prep\n",
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "596cf0c744ce4300bff883c339002907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0.0010,     0.0001,     0.0007,     0.0063,     0.0047,     0.0055,\n",
      "             0.0104,     0.0362,     0.0115,     0.0012,     0.0009,     0.0015,\n",
      "             0.0042,     0.0151,     0.0151,     0.0176,     0.0075,     0.0384,\n",
      "             0.0151,     0.0349,     0.0146,     0.0215,     0.0000,     0.0000],\n",
      "        [    0.0024,     0.0007,     0.0039,     0.0005,     0.0002,     0.0004,\n",
      "             0.0123,     0.0043,     0.0045,     0.0055,     0.0009,     0.0039,\n",
      "             0.0003,     0.0024,     0.0002,     0.0001,     0.0012,     0.0015,\n",
      "             0.0005,     0.0002,     0.0005,     0.0259,     0.0026,     0.0066],\n",
      "        [    0.0003,     0.0006,     0.0002,     0.0066,     0.0085,     0.0012,\n",
      "             0.0002,     0.0002,     0.0021,     0.0526,     0.0217,     0.0074,\n",
      "             0.0008,     0.0009,     0.0181,     0.0832,     0.2899,     0.1086,\n",
      "             0.0089,     0.1473,     0.0500,     0.0005,     0.0040,     0.0004],\n",
      "        [    0.0141,     0.0033,     0.0024,     0.0005,     0.0014,     0.0007,\n",
      "             0.0020,     0.0005,     0.0007,     0.0071,     0.0048,     0.0063,\n",
      "             0.0002,     0.0003,     0.0024,     0.0064,     0.0116,     0.0053,\n",
      "             0.0017,     0.0038,     0.0081,     0.0072,     0.0138,     0.0130],\n",
      "        [    0.0035,     0.0040,     0.0041,     0.0026,     0.0038,     0.0073,\n",
      "             0.0073,     0.0068,     0.0046,     0.0005,     0.0027,     0.0006,\n",
      "             0.0018,     0.0027,     0.0062,     0.0029,     0.0007,     0.0043,\n",
      "             0.0007,     0.0042,     0.0043,     0.0015,     0.0003,     0.0025],\n",
      "        [    0.0015,     0.0027,     0.0030,     0.0151,     0.0108,     0.0268,\n",
      "             0.0736,     0.0038,     0.2552,     0.0040,     0.0010,     0.0040,\n",
      "             0.0310,     0.0098,     0.0107,     0.0048,     0.0037,     0.0103,\n",
      "             0.0032,     0.0036,     0.0084,     0.0073,     0.0018,     0.0033],\n",
      "        [    0.0069,     0.0245,     0.0071,     0.0065,     0.0085,     0.0100,\n",
      "             0.0010,     0.0006,     0.0019,     0.0068,     0.0119,     0.0057,\n",
      "             0.0014,     0.0049,     0.0018,     0.0164,     0.0168,     0.0175,\n",
      "             0.0080,     0.0095,     0.0080,     0.0218,     0.0608,     0.0060],\n",
      "        [    0.0057,     0.0165,     0.0184,     0.0086,     0.0145,     0.0120,\n",
      "             0.0197,     0.0182,     0.0166,     0.0055,     0.0169,     0.0165,\n",
      "             0.0045,     0.0093,     0.0151,     0.0095,     0.0129,     0.0122,\n",
      "             0.0831,     0.0275,     0.0356,     0.0151,     0.0136,     0.0106],\n",
      "        [    0.0131,     0.0221,     0.0692,     0.0134,     0.0230,     0.0140,\n",
      "             0.0260,     0.0056,     0.0322,     0.0177,     0.0106,     0.0161,\n",
      "             0.0164,     0.0054,     0.0178,     0.0063,     0.0055,     0.0060,\n",
      "             0.0237,     0.0116,     0.0125,     0.0063,     0.0116,     0.0221],\n",
      "        [    0.0237,     0.0253,     0.0065,     0.0119,     0.0132,     0.0034,\n",
      "             0.0064,     0.0092,     0.0052,     0.0130,     0.0164,     0.0223,\n",
      "             0.0061,     0.0115,     0.0146,     0.0070,     0.0096,     0.0120,\n",
      "             0.0100,     0.0435,     0.0127,     0.0249,     0.0083,     0.0256],\n",
      "        [    0.0033,     0.0058,     0.0091,     0.1034,     0.0471,     0.0402,\n",
      "             0.0374,     0.0309,     0.0211,     0.0204,     0.0340,     0.0107,\n",
      "             0.0031,     0.0066,     0.0134,     0.0114,     0.0098,     0.0175,\n",
      "             0.0217,     0.0154,     0.0152,     0.0146,     0.0117,     0.0081],\n",
      "        [    0.0043,     0.0180,     0.0267,     0.0170,     0.0178,     0.0144,\n",
      "             0.0108,     0.0182,     0.0234,     0.0202,     0.0244,     0.0156,\n",
      "             0.0126,     0.0024,     0.0014,     0.0092,     0.0122,     0.0201,\n",
      "             0.0237,     0.0134,     0.0137,     0.0553,     0.0164,     0.0281],\n",
      "        [    0.0267,     0.0394,     0.0242,     0.0159,     0.0081,     0.0216,\n",
      "             0.0537,     0.0295,     0.0275,     0.0233,     0.0249,     0.0170,\n",
      "             0.0427,     0.0193,     0.0354,     0.0215,     0.0281,     0.0077,\n",
      "             0.0195,     0.0109,     0.0787,     0.0137,     0.0306,     0.0130],\n",
      "        [    0.0170,     0.0284,     0.0141,     0.0388,     0.0083,     0.0235,\n",
      "             0.0009,     0.0130,     0.0069,     0.0366,     0.0295,     0.0156,\n",
      "             0.0443,     0.0584,     0.0128,     0.0422,     0.0627,     0.0226,\n",
      "             0.0306,     0.0095,     0.0114,     0.0217,     0.0251,     0.0492],\n",
      "        [    0.0416,     0.1008,     0.1931,     0.0246,     0.0111,     0.0195,\n",
      "             0.0091,     0.0308,     0.0173,     0.0138,     0.0896,     0.0414,\n",
      "             0.0608,     0.0182,     0.0128,     0.0418,     0.0752,     0.0466,\n",
      "             0.0200,     0.0414,     0.0164,     0.0597,     0.4686,     0.0035],\n",
      "        [    0.0358,     0.0138,     0.0277,     0.0212,     0.0080,     0.0263,\n",
      "             0.0160,     0.0190,     0.0082,     0.0251,     0.0119,     0.0396,\n",
      "             0.0295,     0.0111,     0.0028,     0.0422,     0.0169,     0.0470,\n",
      "             0.0200,     0.0143,     0.0419,     0.0110,     0.0270,     0.0172],\n",
      "        [    0.0021,     0.0079,     0.0032,     0.0322,     0.0088,     0.0061,\n",
      "             0.0366,     0.0147,     0.0072,     0.0050,     0.0020,     0.0361,\n",
      "             0.0216,     0.0060,     0.0119,     0.0113,     0.0071,     0.0097,\n",
      "             0.0114,     0.0036,     0.0083,     0.0439,     0.0940,     0.0716],\n",
      "        [    0.1548,     0.0483,     0.1108,     0.0062,     0.0056,     0.0015,\n",
      "             0.0159,     0.0071,     0.0084,     0.1834,     0.0432,     0.0080,\n",
      "             0.0025,     0.0043,     0.0095,     0.0326,     0.0037,     0.0350,\n",
      "             0.0133,     0.0038,     0.0063,     0.0015,     0.0022,     0.0011],\n",
      "        [    0.0108,     0.0564,     0.0047,     0.0004,     0.0015,     0.0008,\n",
      "             0.0032,     0.0139,     0.0130,     0.0194,     0.0532,     0.1132,\n",
      "             0.0006,     0.0011,     0.0017,     0.0759,     0.0133,     0.0172,\n",
      "             0.0186,     0.0301,     0.0190,     0.0034,     0.0136,     0.0076],\n",
      "        [    0.0361,     0.0153,     0.0155,     0.0004,     0.0011,     0.0006,\n",
      "             0.0018,     0.0013,     0.0319,     0.0105,     0.0069,     0.0109,\n",
      "             0.0006,     0.0038,     0.0018,     0.0081,     0.0053,     0.0203,\n",
      "             0.0136,     0.0139,     0.0161,     0.0080,     0.0102,     0.0064],\n",
      "        [    0.0016,     0.0023,     0.0015,     0.0108,     0.0086,     0.0204,\n",
      "             0.0100,     0.0030,     0.0029,     0.0219,     0.0196,     0.0064,\n",
      "             0.0009,     0.0039,     0.0142,     0.0081,     0.0093,     0.0319,\n",
      "             0.0533,     0.0262,     0.0204,     0.0012,     0.0008,     0.0005],\n",
      "        [    0.0041,     0.0167,     0.0112,     0.0003,     0.0001,     0.0005,\n",
      "             0.0005,     0.0003,     0.0004,     0.0012,     0.0037,     0.0011,\n",
      "             0.0040,     0.0044,     0.0220,     0.0067,     0.0092,     0.0027,\n",
      "             0.0378,     0.0171,     0.1341,     0.0006,     0.0006,     0.0006],\n",
      "        [    0.0005,     0.0009,     0.0016,     0.0108,     0.0049,     0.0103,\n",
      "             0.0006,     0.0009,     0.0004,     0.0077,     0.0327,     0.0052,\n",
      "             0.0029,     0.0046,     0.0147,     0.0003,     0.0001,     0.0002,\n",
      "             0.0007,     0.0005,     0.0010,     0.0004,     0.0001,     0.0001],\n",
      "        [    0.0068,     0.0159,     0.0090,     0.0009,     0.0015,     0.0022,\n",
      "             0.0030,     0.0024,     0.0198,     0.0119,     0.0703,     0.0097,\n",
      "             0.0004,     0.0009,     0.0183,     0.0032,     0.0035,     0.0070,\n",
      "             0.0032,     0.0008,     0.0133,     0.0239,     0.0205,     0.0263],\n",
      "        [    0.0022,     0.0139,     0.0043,     0.0560,     0.0388,     0.1819,\n",
      "             0.0093,     0.0021,     0.0024,     0.0036,     0.0065,     0.0047,\n",
      "             0.0230,     0.0071,     0.0011,     0.0326,     0.0630,     0.0170,\n",
      "             0.0009,     0.0008,     0.0009,     0.0097,     0.0120,     0.0030],\n",
      "        [    0.0050,     0.0007,     0.0016,     0.0011,     0.0019,     0.0005,\n",
      "             0.0157,     0.0126,     0.0260,     0.0003,     0.0011,     0.0003,\n",
      "             0.0017,     0.0029,     0.0016,     0.0365,     0.0341,     0.0057,\n",
      "             0.0000,     0.0001,     0.0002,     0.0150,     0.0012,     0.0170],\n",
      "        [    0.0116,     0.0158,     0.0214,     0.0028,     0.0038,     0.0006,\n",
      "             0.0198,     0.0266,     0.0729,     0.0522,     0.0185,     0.0466,\n",
      "             0.0204,     0.0221,     0.0212,     0.0140,     0.0679,     0.0049,\n",
      "             0.0116,     0.0141,     0.0256,     0.0325,     0.0476,     0.0560],\n",
      "        [    0.0044,     0.0021,     0.0020,     0.0045,     0.0032,     0.0199,\n",
      "             0.0005,     0.0007,     0.0012,     0.0075,     0.0320,     0.0006,\n",
      "             0.0209,     0.0114,     0.0072,     0.0229,     0.0223,     0.0222,\n",
      "             0.0009,     0.0029,     0.0016,     0.0212,     0.0400,     0.0802]])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'indices_induction_heads' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[163], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour current working directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_experiment_suite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_csv_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCSV_PATH_DATASET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_repetitions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[162], line 9\u001b[0m, in \u001b[0;36mrun_experiment_suite\u001b[0;34m(dataset_csv_file_path, llm_models, prompt_repetitions)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use apply() in a vectorized manner to pass both columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m dataset\n\u001b[0;32m----> 9\u001b[0m dataset[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_true_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_false_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Drop rows where `process_row()` returned NaN\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_true_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_false_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/helmholtz/lib/python3.10/site-packages/pandas/core/frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10373\u001b[0m )\n\u001b[0;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/helmholtz/lib/python3.10/site-packages/pandas/core/apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[0;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/helmholtz/lib/python3.10/site-packages/pandas/core/apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/helmholtz/lib/python3.10/site-packages/pandas/core/apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[162], line 10\u001b[0m, in \u001b[0;36mrun_experiment_suite.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Use apply() in a vectorized manner to pass both columns\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m dataset\n\u001b[1;32m      9\u001b[0m dataset[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_true_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_false_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mprocess_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     11\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Drop rows where `process_row()` returned NaN\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dataset\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_true_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_probability_false_sentence\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[161], line 3\u001b[0m, in \u001b[0;36mprocess_row\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_row\u001b[39m(row):\n\u001b[0;32m----> 3\u001b[0m     result_true, result_false \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_sentence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue_sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mfalse_sentence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfalse_sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Check if result is None or invalid\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result_true \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[160], line 18\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(true_sentence, false_sentence)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(induction_scores)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# create_heatmap(induction_scores=induction_scores)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# indices_induction_heads = sort_filter_high_scoring_induction_heads(induction_scores=induction_scores, model_output=models_output)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Extract the probability of the 2 studied tokens for each head and layer. Store it in json format  \u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m token_probability_true_sentence, token_probability_false_sentence \u001b[38;5;241m=\u001b[39m token_probability_extraction(\u001b[43mindices_induction_heads\u001b[49m, models_output, token_number_sentence)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m token_probability_true_sentence, token_probability_false_sentence\n",
      "\u001b[0;31mNameError\u001b[0m: name 'indices_induction_heads' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Your current working directory:\", os.getcwd())\n",
    "run_experiment_suite(dataset_csv_file_path=CSV_PATH_DATASET, llm_models=models, prompt_repetitions=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
